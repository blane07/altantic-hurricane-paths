{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Path Predictor\n",
    "The HURDAT2 dataset is from the US National Hurricane Center. It is a collection of Atlantic hurricanes that occured from 1851 to 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .csv file line-by-line\n",
    "with open('hurdat2-1851-2016-041117.csv', 'r') as f: \n",
    "       file_data = [row.strip().split(',') for row in f]\n",
    "# Convert data file to data frame\n",
    "df = pd.DataFrame(file_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns with missing or incomplete data\n",
    "df.drop([2] + list(range(7,21)), axis='columns', inplace=True)\n",
    "# Renaming columns\n",
    "data = pd.DataFrame(np.matrix(df),\n",
    "                    columns=['date', 'time', 'status', 'lat', 'lon', 'max_wind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding new events\n",
    "def time_convert(time_str):\n",
    "    try:\n",
    "        return(int(time_str.strip()))\n",
    "    except:\n",
    "        pass\n",
    "data.date = data.date.apply(lambda x: '99999999' if x[0] == 'A' else x.strip())\n",
    "data.time = data.time.apply(lambda x: x.strip())\n",
    "data.status = data.status.apply(lambda x: 'NE' if x == '' else x.strip())\n",
    "data.lat = data.lat.apply(lambda x: '00.0N' if x is None else x.strip())\n",
    "data.lon = data.lon.apply(lambda x: '00.0W' if x is None else x.strip())\n",
    "data.max_wind = data.max_wind.apply(lambda x: 0 if x is None else int(x.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N'}\n"
     ]
    }
   ],
   "source": [
    "# Only 'North' latitudes in the data set\n",
    "lst = []\n",
    "for x in data.lat:\n",
    "    if x is not None:\n",
    "        lst.append(x[-1])\n",
    "lst = set(lst)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W', 'E'}\n"
     ]
    }
   ],
   "source": [
    "# 'West' and 'East' longitudes in the data set\n",
    "lst = []\n",
    "for x in data.lon:\n",
    "    if x is not None:\n",
    "        lst.append(x[-1])\n",
    "lst = set(lst)\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting latitude to float data type\n",
    "data.lat = data.lat.apply(lambda x: float(x.strip()[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting longitude to float data type\n",
    "def lon_convert(lon):\n",
    "    lon = lon.strip()\n",
    "    lon_sign = 1\n",
    "    if lon[-1] == 'W':\n",
    "        lon_sign = -1\n",
    "    return(lon_sign * float(lon[:-1]))\n",
    "\n",
    "data.lon = data.lon.apply(lambda x: lon_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying noise in the time column\n",
    "def time_filter(t):\n",
    "    try:\n",
    "        int(t)\n",
    "        return(t)\n",
    "    except:\n",
    "        return('9999')\n",
    "\n",
    "data.time = data.time.apply(lambda x: time_filter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date information\n",
    "data.loc[:, 'year'] = data.date.apply(lambda x: int(x[0:4]))\n",
    "data.loc[:, 'month'] = data.date.apply(lambda x: int(x[4:6]))\n",
    "data.loc[:, 'day'] = data.date.apply(lambda x: int(x[6:8]))\n",
    "data.drop('date', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time information\n",
    "data.loc[:, 'hour'] = data.time.apply(lambda x: int(x[0:2]))\n",
    "data.loc[:, 'minute'] = data.time.apply(lambda x: int(x[2:4]))\n",
    "data.drop('time', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert current time to an hour decimal\n",
    "time_hour_decimal = np.array(data.hour) + np.array(data.minute) / 60\n",
    "# Time delta in hours\n",
    "time_delta = time_hour_decimal - np.roll(time_hour_decimal, 1)\n",
    "# Correcting time deltas that occur across midnight\n",
    "time_delta = [24 + x if x < 0 else x for x in time_delta]\n",
    "data.loc[:, 'time_delta'] = time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created latitude and longitude rate of change features\n",
    "data.loc[:, 'lat_vel'] = (np.array(data.lat) - np.roll(np.array(data.lat), 1)) / time_delta\n",
    "# data.loc[:, 'lat_acc'] = np.round((np.array(data.lat_vel) - \n",
    "#                           np.roll(np.array(data.lat_vel), 1)) / time_delta, 8)\n",
    "data.loc[:, 'lon_vel'] = (np.array(data.lon) - np.roll(np.array(data.lon), 1)) / time_delta\n",
    "# data.loc[:, 'lon_acc'] = np.round((np.array(data.lon_vel) - \n",
    "#                           np.roll(np.array(data.lon_vel), 1)) / time_delta, 8)\n",
    "# Filter out bogus results\n",
    "data.lat_vel = data.lat_vel.apply(lambda x: 0 if abs(x) > 0.1 else x)\n",
    "# data.lat_acc = data.lat_acc.apply(lambda x: 0 if abs(x) > 3 else x)\n",
    "data.lon_vel = data.lon_vel.apply(lambda x: 0 if abs(x) > 0.1 else x)\n",
    "# data.lon_acc = data.lon_acc.apply(lambda x: 0 if abs(x) > 3 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture transitions in storm's status\n",
    "data.loc[:, 'prev_status'] = np.roll(np.array(data.status), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary categorical columns\n",
    "data = pd.concat([data,\n",
    "                  pd.get_dummies(data.status),\n",
    "                  pd.get_dummies(data.prev_status),\n",
    "                  pd.get_dummies(data.month, prefix='month'),\n",
    "                  pd.get_dummies(data.day, prefix='day'),\n",
    "                  pd.get_dummies(data.hour, prefix='hour')],\n",
    "                axis='columns')\n",
    "# Deleting redundant columns\n",
    "data.drop(['status', 'prev_status', 'month', 'day', 'hour'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct first entry values\n",
    "data.time_delta = data.time_delta.apply(lambda x: 0 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lat', 'lon', 'max_wind', 'year', 'minute', 'time_delta', 'lat_vel',\n",
       "       'lon_vel', 'DB', 'EX', 'HU', 'LO', 'NE', 'SD', 'SS', 'TD', 'TS', 'WV',\n",
       "       'DB', 'EX', 'HU', 'LO', 'NE', 'SD', 'SS', 'TD', 'TS', 'WV', 'month_1',\n",
       "       'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7',\n",
       "       'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'month_99',\n",
       "       'day_1', 'day_2', 'day_3', 'day_4', 'day_5', 'day_6', 'day_7', 'day_8',\n",
       "       'day_9', 'day_10', 'day_11', 'day_12', 'day_13', 'day_14', 'day_15',\n",
       "       'day_16', 'day_17', 'day_18', 'day_19', 'day_20', 'day_21', 'day_22',\n",
       "       'day_23', 'day_24', 'day_25', 'day_26', 'day_27', 'day_28', 'day_29',\n",
       "       'day_30', 'day_31', 'day_99', 'hour_0', 'hour_1', 'hour_2', 'hour_3',\n",
       "       'hour_4', 'hour_5', 'hour_6', 'hour_7', 'hour_8', 'hour_9', 'hour_10',\n",
       "       'hour_11', 'hour_12', 'hour_13', 'hour_14', 'hour_15', 'hour_16',\n",
       "       'hour_17', 'hour_18', 'hour_19', 'hour_20', 'hour_21', 'hour_22',\n",
       "       'hour_23', 'hour_99'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next position (lat, lon)\n",
    "label = pd.DataFrame(np.roll(np.array(data[['lat', 'lon']]), [-1,-1]),\n",
    "                     columns=['lat_pred', 'lat_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into a test set and a training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the inputs\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "model = Sequential()\n",
    "n_input = X_train.shape[1]\n",
    "n_hidden = n_input\n",
    "# Topology\n",
    "model.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "istory = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "              epochs=100, batch_size=None, verbose=0)\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training loss')\n",
    "plt.plot(test_loss, label='Testing loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
